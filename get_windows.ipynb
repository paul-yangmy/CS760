{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from gensim import corpora, models\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import preprocess, get_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNTS = 20\n",
    "MAX_COUNTS = 1800\n",
    "# words with count < MIN_COUNTS\n",
    "# and count > MAX_COUNTS\n",
    "# will be removed\n",
    "\n",
    "MIN_LENGTH = 15\n",
    "# minimum document length \n",
    "# (number of words)\n",
    "# after preprocessing\n",
    "\n",
    "# half the size of the context around a word\n",
    "HALF_WINDOW_SIZE = 5\n",
    "# it must be that 2*HALF_WINDOW_SIZE < MIN_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load NLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "docs = dataset['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "18846"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of documents\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store an index with a document\n",
    "docs = [(i, doc) for i, doc in enumerate(docs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess dataset and create windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18846/18846 [10:38<00:00, 29.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of removed short documents: 3975\n",
      "total number of tokens: 1442761\n",
      "number of tokens to be removed: 417459\n",
      "number of additionally removed short documents: 2185\n",
      "total number of tokens: 1000435\n",
      "\n",
      "minimum word count number: 15\n",
      "this number can be less than MIN_COUNTS because of document removal\n"
     ]
    }
   ],
   "source": [
    "encoded_docs, decoder, word_counts = preprocess(\n",
    "    docs, nlp, MIN_LENGTH, MIN_COUNTS, MAX_COUNTS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# new ids will be created for the documents.\n",
    "# create a way of restoring initial ids:\n",
    "doc_decoder = {i: doc_id for i, (doc_id, doc) in enumerate(encoded_docs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12686it [00:01, 7514.94it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "# new ids are created here\n",
    "for index, (_, doc) in tqdm(enumerate(encoded_docs)):\n",
    "    windows = get_windows(doc, HALF_WINDOW_SIZE)\n",
    "    # index represents id of a document, \n",
    "    # windows is a list of (word, window around this word),\n",
    "    # where word is in the document\n",
    "    data += [[index, w[0]] + w[1] for w in windows]\n",
    "\n",
    "data = np.array(data, dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "12"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a row in 'data' contains:\n",
    "# id of a document, id of a word in this document, a window around this word\n",
    "# 1 + 1 + 10\n",
    "data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1000435"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of windows (equals to the total number of tokens)\n",
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get unigram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = np.array(word_counts)\n",
    "unigram_distribution = word_counts/sum(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab_size = len(decoder)\n",
    "embedding_dim = 50\n",
    "\n",
    "# train a skip-gram word2vec model\n",
    "texts = [[str(j) for j in doc] for i, doc in encoded_docs]\n",
    "model = models.Word2Vec(texts, size=embedding_dim, window=5, workers=4, sg=1, negative=15, iter=70)\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "word_vectors = np.zeros((vocab_size, embedding_dim)).astype('float32')\n",
    "for i in decoder:\n",
    "    word_vectors[i] = model.wv[str(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "7863"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of unique words\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Prepare initialization for document weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [[decoder[j] for j in doc] for i, doc in encoded_docs]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_topics = 25\n",
    "lda = models.LdaModel(corpus, alpha=0.9, id2word=dictionary, num_topics=n_topics)\n",
    "corpus_lda = lda[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 : food taste eat MSG lot Serbs sure little place Muslims\n",
      "topic 1 : fire FBI shoot kill tank weapon Koresh attack report child\n",
      "topic 2 : law public President encryption gun state firearm private job chip\n",
      "topic 3 : New cover appear book price Turkey Press copy York art\n",
      "topic 4 : pin latch modem input jumper logic current slave Symbol connect\n",
      "topic 5 : probably idea lot chip sure pretty far hear order ago\n",
      "topic 6 : University gun article Wright Professor rate quote study crime James\n",
      "topic 7 : research science age group Center material National service scientific conference\n",
      "topic 8 : evidence reason cause mind bad life claim happen answer course\n",
      "topic 9 : space launch Earth orbit NASA planet satellite mission solar Space\n",
      "topic 10 : Mary probably great happen bad far sure Latest end place\n",
      "topic 11 : bike battery engine bad buy sure ride bit light little\n",
      "topic 12 : man Jesus Christ Lord armenian turkish Armenians child greek sin\n",
      "topic 13 : card disk driver color Windows software board monitor controller Mac\n",
      "topic 14 : Yassin Deir pay computer sure price maybe scare place sell\n",
      "topic 15 : window user version application list mail server display software code\n",
      "topic 16 : Israel Jews israeli jewish arab country war state peace attack\n",
      "topic 17 : belief religion Bible word church atheist argument evidence claim faith\n",
      "topic 18 : Kinsey drug patient child report cause disease treatment increase health\n",
      "topic 19 : section error bit echo build entry copy input output DOS\n",
      "topic 20 : car buy leave door old happen hear live home turn\n",
      "topic 21 : wire ground power circuit cable box ADL wiring outlet neutral\n",
      "topic 22 : pitch win hit pitcher lot average team fan baseball guy\n",
      "topic 23 : team play player season win NHL score goal period hockey\n",
      "topic 24 : RIPEM message detector DES PGP radar bit distribution public PEM\n"
     ]
    }
   ],
   "source": [
    "for i, topics in lda.show_topics(n_topics, formatted=False):\n",
    "    print('topic', i, ':', ' '.join([t for t, _ in topics]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12686/12686 [00:07<00:00, 1679.65it/s]\n"
     ]
    }
   ],
   "source": [
    "doc_weights_init = np.zeros((len(corpus_lda), n_topics))\n",
    "for i in tqdm(range(len(corpus_lda))):\n",
    "    topics = corpus_lda[i]\n",
    "    for j, prob in topics:\n",
    "        doc_weights_init[i, j] = prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('data.npy', data)\n",
    "np.save('word_vectors.npy', word_vectors)\n",
    "np.save('unigram_distribution.npy', unigram_distribution)\n",
    "np.save('decoder.npy', decoder)\n",
    "np.save('doc_decoder.npy', doc_decoder)\n",
    "np.save('doc_weights_init.npy', doc_weights_init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}